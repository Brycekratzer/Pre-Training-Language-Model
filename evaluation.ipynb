{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Model Evaluation for Pre Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import transformers\n",
    "\n",
    "# BERT Model is what we are comparing too\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Used for importing our pretrained model\n",
    "from transformers import ElectraTokenizerFast\n",
    "from transformers import ElectraConfig\n",
    "from transformers import ElectraModel\n",
    "from transformers import ElectraForMaskedLM\n",
    "\n",
    "import torch\n",
    "\n",
    "# Can use with NVIDIA Cuda\n",
    "from torch import cuda\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# Model was fine tuned and evaluated using Mac's Metal 3 GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46985, 46985, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in data we will use to fine tune and evaluate model with\n",
    "X = [line.strip() for line in open('X.txt').readlines()]\n",
    "y = train_data = [int(line.strip()) for line in open('YL1.txt').readlines()]\n",
    "\n",
    "len(X), len(y), max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually split our data into a proper train and test set. We also allow for us to take a sample of our data to edit/modify the pipeline\n",
    "without going through the large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4490, 4490, 110, 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix constant for size of data\n",
    "full_data = 46000\n",
    "split_percent = .1\n",
    "\n",
    "# taking 10% of full data\n",
    "sample_data = int(full_data * split_percent)\n",
    "\n",
    "# Used for indexed split\n",
    "sample_split = (sample_data - int(sample_data * .024))\n",
    "\n",
    "# Taking a sample of the original data\n",
    "sample_X = X[:sample_data] \n",
    "sample_y = y[:sample_data]\n",
    "\n",
    "# Full test/train\n",
    "train_X = X[:46000]\n",
    "train_y = np.array(y[:46000])\n",
    "test_X = X[46000:]\n",
    "test_y = np.array(y[46000:])\n",
    "\n",
    "# Sample test/train\n",
    "sample_train_X = sample_X[:sample_split]\n",
    "sample_train_y = np.array(sample_y[:sample_split])\n",
    "sample_test_X = sample_X[sample_split:]\n",
    "sample_test_y = np.array(sample_y[sample_split:])\n",
    "\n",
    "len(train_X), len(train_y), len(test_X), len(test_y)\n",
    "len(sample_train_X), len(sample_train_y), len(sample_test_X), len(sample_test_y), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up our labels for what documents we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not needed for training or evaluation, but useful for mapping examples\n",
    "labels = {\n",
    "    0:'Computer Science',\n",
    "    1:'Electrical Engineering',\n",
    "    2:'Psychology',\n",
    "    3:'Mechanical Engineering',\n",
    "    4:'Civil Engineering',\n",
    "    5:'Medical Science',\n",
    "    6:'Biochemistry'\n",
    "}\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune BERT on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class implements a custom dataset for multi label text classification in PyTorch. In multi label classification, each text example can belong to multiple categories simultaneously (unlike single label classification where each example belongs to exactly one class).\n",
    "\n",
    "The core purpose is to prepare text data for input into our models for evaluation by:\n",
    "\n",
    "- Converting raw text into tokenized numerical representations\n",
    "- Handling multiple target labels for each text example\n",
    "- Formatting everything in a way PyTorch can efficiently process during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding pre-trained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BERTClass` sets up the BERT model using the pre-trained `bert-base-uncased` model from hugging face. `NUM_OUT` is the number of outputs available to the model. In our case this number is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(BERTClass, self).__init__()\n",
    "                   \n",
    "        self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.classifier = torch.nn.Linear(768, NUM_OUT)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding pre-trained ELECTRA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ELECTRAClass` is similar to the pre-trained BERT Model, but we are importing our model from the directory rather than hugging face. Sense we used masked wording for our pre-training we need to bypass the initial output of the model and pass the final layer before our prediction to a classifier for our evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(ELECTRAClass, self).__init__()\n",
    "        # Load in configurations\n",
    "        self.config = ElectraConfig.from_json_file(\"config.json\")\n",
    "        \n",
    "        # Use ElectraModel instead of ElectraForMaskedLM\n",
    "        self.l1 = ElectraForMaskedLM(self.config)\n",
    "        \n",
    "        # Load in model with pre trained weights\n",
    "        state_dict = torch.load(\"babylm_model.bin\", map_location=torch.device('cpu'))\n",
    "\n",
    "        # Loading weights into model configerations\n",
    "        self.l1.load_state_dict(state_dict)\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(self.config.hidden_size, NUM_OUT)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        # Inputs the data to the encoder layers of electra then bypassing the \n",
    "        # mask prediction.\n",
    "        output_1 = self.l1.electra(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        # Grabs out outputs from the last encoder layer\n",
    "        hidden_state = output_1.last_hidden_state \n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # Pass our vectors to classifier for final prediction\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model layout for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELECTRAClass(\n",
       "  (l1): ElectraForMaskedLM(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 64, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 64)\n",
       "        (token_type_embeddings): Embedding(2, 64)\n",
       "        (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=64, out_features=196, bias=True)\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-17): 18 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=196, out_features=196, bias=True)\n",
       "                (key): Linear(in_features=196, out_features=196, bias=True)\n",
       "                (value): Linear(in_features=196, out_features=196, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=196, out_features=196, bias=True)\n",
       "                (LayerNorm): LayerNorm((196,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=196, out_features=128, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=128, out_features=196, bias=True)\n",
       "              (LayerNorm): LayerNorm((196,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator_predictions): ElectraGeneratorPredictions(\n",
       "      (activation): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "      (dense): Linear(in_features=196, out_features=64, bias=True)\n",
       "    )\n",
       "    (generator_lm_head): Linear(in_features=64, out_features=30522, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=196, out_features=7, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ELECTRAClass(NUM_OUT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(label_smoothing=0.001)(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testing_loader):\n",
    "            targets = data['targets']\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.softmax(outputs, dim=1).cpu().detach()\n",
    "            fin_outputs.extend(outputs)\n",
    "            fin_targets.extend(targets)\n",
    "    return torch.stack(fin_outputs), torch.stack(fin_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our Base Bert Model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# For our Electra model\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-base-discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters were gathered from trial and error of finding what has the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64 # 64\n",
    "BATCH_SIZE = 16 # Try 32\n",
    "EPOCHS = 3\n",
    "NUM_OUT = 7\n",
    "LEARNING_RATE = 1e-05 #1e-05\n",
    "\n",
    "training_data = MultiLabelDataset(train_X, torch.from_numpy(train_y), tokenizer, MAX_LEN)\n",
    "test_data = MultiLabelDataset(test_X, torch.from_numpy(test_y), tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }    \n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, **train_params)\n",
    "testing_loader = torch.utils.data.DataLoader(test_data, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2875 [00:00<?, ?it/s]/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_65800/194539289.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
      "100%|███████████████████████████████████████| 2875/2875 [06:52<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  1.8762006759643555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 62/62 [00:01<00:00, 38.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arracy on test set 0.1736040609137056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2875 [00:00<?, ?it/s]/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_65800/194539289.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
      "100%|███████████████████████████████████████| 2875/2875 [06:50<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  1.663584589958191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 62/62 [00:01<00:00, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arracy on test set 0.38578680203045684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2875 [00:00<?, ?it/s]/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/dz/ls7s785d52j9jwwk1cpbk5fr0000gn/T/ipykernel_65800/194539289.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
      "100%|███████████████████████████████████████| 2875/2875 [06:54<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  1.5776500701904297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 62/62 [00:01<00:00, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arracy on test set 0.515736040609137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = BERTClass(NUM_OUT)\n",
    "model = ELECTRAClass(NUM_OUT)\n",
    "model.to(device)    \n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, training_loader, optimizer)\n",
    "    print(f'Epoch: {epoch + 1}, Loss:  {loss}')  \n",
    "    guess, targs = validation(model, testing_loader)\n",
    "    guesses = torch.argmax(guess, dim=1)\n",
    "    print('arracy on test set {}'.format(accuracy_score(y_pred=guesses, y_true=targs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Performance on batch size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EPOCH 1**\n",
    "\n",
    "BERT 75% Accuracy | Loss 1.39\n",
    "\n",
    "ELECTRA 57% | Loss 1.49\n",
    "\n",
    "**EPOCH 2**\n",
    "\n",
    "BERT 78% | Loss 1.29\n",
    "\n",
    "ELECTRA 69% | Loss 1.39\n",
    "\n",
    "**EPOCH 3**\n",
    "\n",
    "BERT 77% | Loss 1.29\n",
    "\n",
    "ELECTRA 70%  | Loss 1.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on batch 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EPOCH 1**\n",
    "\n",
    "BERT 77% \n",
    "\n",
    "ELECTRA 17% | Loss 1.87\n",
    "\n",
    "**EPOCH 2**\n",
    "\n",
    "BERT 77%\n",
    "\n",
    "ELECTRA 38% | Loss 1.66\n",
    "\n",
    "**EPOCH 8**\n",
    "\n",
    "BERT 78%\n",
    "\n",
    "ELECTRA 52% | Loss 1.58\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Performance on batch 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EPOCH 1**\n",
    "\n",
    "BERT 75% | Loss 1.4\n",
    "\n",
    "TinyBERT 66% | Loss 1.41\n",
    "\n",
    "ELECTRA 17% | Loss 1.9\n",
    "\n",
    "**EPOCH 2**\n",
    "\n",
    "BERT 78% | Loss 1.39\n",
    "\n",
    "TinyBERT 67% | Loss 1.37\n",
    "\n",
    "ELECTRA 17% | Loss 1.8\n",
    "\n",
    "**EPOCH 2**\n",
    "\n",
    "BERT 78% | Loss 1.28\n",
    "\n",
    "TinyBERT 70% | Loss 1.41\n",
    "\n",
    "ELECTRA 17% | Loss 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Summary of Batch size for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Size Increase for BERT Model**\n",
    "\n",
    "With a batch size of *8* the model:\n",
    "\n",
    "- Tended to improve very well after the first epoch\n",
    "- Performance stayed the same and fell a tad bit after the 3rd epoch\n",
    "\n",
    "With a batch size of *16* the model:\n",
    "\n",
    "- Had a non-volatile increase from the first to the last epoch\n",
    "- Had very similar results as a batch size of 8 but stayed more consistent\n",
    "- Increased gradually and didn't decrease\n",
    "\n",
    "With a batch size of *32* the model:\n",
    "\n",
    "- Was very similar to a batch size of 8 in terms of volatility\n",
    "- Had a higher, more consistent accuracy score at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of ELECTRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Size Increase for ELECTRA Model**\n",
    "\n",
    "With a batch size of *8* the model:\n",
    "\n",
    "- Improved significantly compared to the previous batch size's\n",
    "- Performance increased had a logarithmic growth, with increased improvement from epoch 1 to epoch 2, but only a small improvement from epoch 2 to epoch 3\n",
    "\n",
    "With a batch size of *16* the model:\n",
    "\n",
    "- Had a large improvement from epoch to epoch. \n",
    "- Had similar accuracy at the start when compared to batch 32, but was able to pick up accuracy.\n",
    "\n",
    "With a batch size of *32* the model:\n",
    "\n",
    "- Volatility of improvement was super low\n",
    "- Did not change accuracy at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### BERT vs ELECTRA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT was significantly more consistent when translating across different batch sizes. BERT's accuracy overall was better than ELECTRA's for batch sizes 16 and 32. For batch size 8, ELECTRA slightly caught up to BERT's accuracy but still didn't surpass it.\n",
    "ELECTRA was much faster during the fine-tuning process and showed better accuracy improvements with each epoch, but still didn't perform as well on the task as BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Computation with Fine-Tuning VS Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Training takes significantly more time computing compared to fine-tuning due to the following factors:\n",
    "\n",
    "- Pre-Training:\n",
    "\n",
    "    - Pre-training a model involves starting from complete scratch as we begin with random weights (parameters) and try to guess a word through masking. This involves tuning a significant number of parameters each time we pass through our model and gradually improve the accuracy of word prediction.\n",
    "\n",
    "    - In order to guess these words without any prior context, we need a significant amount of data for reference. As we pass through the model, we analyze a dataset to gather more context to make better predictions.\n",
    "\n",
    "    - A good analogy for this is like starting in preschool. Children have little prior knowledge of concepts relating to academics, so we slowly build on concepts and learn incrementally in order to build a solid foundation on the general context of academia.\n",
    "\n",
    "- Fine-Tuning: \n",
    "\n",
    "    - Fine-tuning a model involves using an existing model and adapting it to improve knowledge in a specific area. Since we already have a solid foundation, we don't need to adjust each parameter as much to achieve better accuracy.\n",
    "\n",
    "    - With more initial context, less data is needed to improve the accuracy for a particular task. This allows for reduced analysis overall when working with datasets.\n",
    "\n",
    "    - A good analogy for this is like starting in college. High school graduates have decent prior knowledge of academic concepts. We can now expand the knowledge we already have on specialized concepts to build a solid foundation in a specific area."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
